# -*- coding: utf-8 -*-
"""AnishKumar_23cs057_Experimment8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PHQYh1zblfISLTlwAWtNyL70JDKL0Rh9
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay

"""***Objective1***"""

X, y = make_moons(n_samples=500, noise=0.25, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.3, random_state=42
)

plt.figure(figsize=(7, 5))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.viridis, s=30, edgecolor='k')
plt.title("Moons Dataset (Non-linear, Noisy)")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

# Step 3: Standardize Features (important for SVM)
scaler = StandardScaler()
scaler.fit(X_train)  # Fit only on training data

X_train_scaled = scaler.transform(X_train)
X_val_scaled = scaler.transform(X_val)

"""Obj 2"""

linear_model = SVC(kernel='linear', C=1.0, random_state=42)
linear_model.fit(X_train_scaled, y_train)

y_pred_linear = linear_model.predict(X_val_scaled)

print("=== Linear SVM Evaluation ===")
print(classification_report(y_val, y_pred_linear))
linear_acc = accuracy_score(y_val, y_pred_linear)
print("Validation Accuracy:", linear_acc, "\n")

"""## Analysis:
- The linear SVM fails to perfectly classify the 'moons' dataset because the data is non-linearly separable.
- The parameter C controls the trade-off between margin width and misclassification.
- A smaller C (like 0.01) makes the model more tolerant to misclassification, increasing the margin but lowering accuracy.

Obj 3
"""

rbf_model = SVC(kernel='rbf', random_state=42)
rbf_model.fit(X_train_scaled, y_train)
y_pred_rbf = rbf_model.predict(X_val_scaled)

print("=== RBF Kernel SVM Evaluation ===")
print(classification_report(y_val, y_pred_rbf))
rbf_acc = accuracy_score(y_val, y_pred_rbf)
print("Validation Accuracy:", rbf_acc, "\n")

poly_model = SVC(kernel='poly', degree=3, random_state=42)
poly_model.fit(X_train_scaled, y_train)
y_pred_poly = poly_model.predict(X_val_scaled)

print("=== Polynomial Kernel SVM Evaluation ===")
print(classification_report(y_val, y_pred_poly))
poly_acc = accuracy_score(y_val, y_pred_poly)
print("Validation Accuracy:", poly_acc, "\n")

print("=== Validation Accuracy Comparison ===")
print(f"{'Model':<20}{'Accuracy':<10}")
print(f"{'-'*30}")
print(f"{'Linear Kernel':<20}{linear_acc:.4f}")
print(f"{'RBF Kernel':<20}{rbf_acc:.4f}")
print(f"{'Polynomial Kernel':<20}{poly_acc:.4f}\n")

"""## Analysis:
- The RBF kernel performs the best with default parameters.
- This makes sense because the 'moons' dataset is non-linear, and the RBF kernel can model curved boundaries effectively.

Obj - 4
"""

param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.1, 1, 10, 100],
    'kernel': ['rbf']
}

grid = GridSearchCV(
    SVC(random_state=42),
    param_grid,
    refit=True,
    verbose=2,
    cv=5,
    scoring='accuracy'
)

print("Running Grid Search... (this may take a few seconds)")
grid.fit(X_train_scaled, y_train)

print("\n=== Grid Search Results ===")
print("Best Parameters:", grid.best_params_)
print("Best Cross-Validated Accuracy:", grid.best_score_, "\n")

"""Task 5"""

final_predictions = grid.predict(X_val_scaled)
print("=== Final Model Evaluation on Hold-Out Set ===")
print(classification_report(y_val, final_predictions))

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_val, final_predictions)

plt.figure(figsize=(6, 5))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues', colorbar=True)

plt.title("Confusion Matrix — Final RBF Model", fontsize=14, pad=15)
plt.xlabel("Predicted Labels", fontsize=12)
plt.ylabel("True Labels", fontsize=12)
plt.grid(False)
plt.show()

def plot_decision_boundary(model, X, y, ax, title):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),
                         np.linspace(y_min, y_max, 300))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    ax.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.coolwarm)
    ax.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k', cmap=plt.cm.coolwarm)
    ax.set_title(title)

fig, axes = plt.subplots(1, 3, figsize=(18, 6), facecolor="#ffffff")

plot_decision_boundary(linear_model, X_train_scaled, y_train, axes[0], "Linear SVM")
axes[0].set_facecolor("#ffe6e6")
axes[0].set_xlabel("Feature 1", fontsize=12)
axes[0].set_ylabel("Feature 2", fontsize=12)
axes[0].set_title("Linear SVM", fontsize=15, color="#e63946", fontweight='bold')

plot_decision_boundary(rbf_model, X_train_scaled, y_train, axes[1], "RBF SVM (Default)")
axes[1].set_facecolor("#fff7cc")
axes[1].set_xlabel("Feature 1", fontsize=12)
axes[1].set_ylabel("Feature 2", fontsize=12)
axes[1].set_title("RBF SVM (Default)", fontsize=15, color="#f4a261", fontweight='bold')

plot_decision_boundary(grid.best_estimator_, X_train_scaled, y_train, axes[2], "RBF SVM (Tuned)")
axes[2].set_facecolor("#e6ffe6")
axes[2].set_xlabel("Feature 1", fontsize=12)
axes[2].set_ylabel("Feature 2", fontsize=12)
axes[2].set_title("RBF SVM (Tuned)", fontsize=15, color="#2a9d8f", fontweight='bold')

fig.suptitle(
    " SVM Decision Boundaries — Linear vs RBF (Default & Tuned)",
    fontsize=20,
    color="#222222",
    fontweight='bold',
    y=0.97
)

for ax in axes:
    ax.grid(True, linestyle='--', alpha=0.5, color='#999999')
    ax.tick_params(colors='#333333')
    for spine in ax.spines.values():
        spine.set_edgecolor('#888888')

plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()